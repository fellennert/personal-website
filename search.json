[
  {
    "objectID": "projects copy.html",
    "href": "projects copy.html",
    "title": "Projects",
    "section": "",
    "text": "This section is where theory meets practice. Each project here represents a different slice of what I bring to data work: collecting messy data from the web, building and comparing machine learning models, applying cutting-edge NLP techniques, creating interactive tools to make my partner happy, and making complex methods accessible to others.\nSome of these grew out of pure curiosity (what patterns emerge in song lyrics? Can we detect “eras”?), others from practical needs (how do I make my teaching materials easily searchable for my students?), and a few from different research challenges (what is the best classifier for a job? how good are LLMs actually? is political polarization really so hot?). Together, they show how I approach problems: I start with a question, pick the right tools and data for the job, and don’t stop until I’ve found an answer worth sharing.\nYou’ll see a mix of techniques here: classic machine learning, modern transformer models, web scraping, interactive dashboards, and RAG applications. But the thread that runs through all of them is the same: how can I turn raw data into understandable insights, into something people can actually understand and use.\n\nPolarization terms\nThis emerged as part of writing the introductory chapter for my dissertation on political polarization. Political polarization has been coined “Word of the Year 2024” by Merriam Webster, reflecting the search traffic on their page. I was wondering whether there would be different ways to document such a surge in attention. Therefore, I moved to Google Trends (“are people interested in political polarization?”), Google Scholar (“are researchers interested in political polarization?”), and the New York Times API (“are journalists interested in political polarization?”). Turns out, yes, there’s been an overall increase in attention. This summary can serve as an example of how to scrape valuable data sources and navigate obstacles such as CAPTCHAs. READ MORE HERE.\n\n\nTraining and comparing ML classifiers\nOne topic of a graduate class I taught on Computational Social Science was supervised classification of text. I showed the students different approaches for doing this (simple, dictionary-based; more advanced, using bag-of-words-based models; and advanced, using BERT). To show the students how capable these different models are, I decided to train and compare several machine learning classifiers. Due to time constraints, I resorted to a pre-labeled data set that was available on Kaggle, containing IMDb reviews of movies. Here’s a little report on my results, with an emphasis on the impact of preprocessing and the number of training examples. READ MORE HERE.\n\n\nBERTopic on survey responses\nFor a research paper, I needed to analyze open-ended survey responses. They came with particular challenges: (a) they were very short, rendering “classic” mixed membership models useless; (b) they came in three different languages (English, German, Swedish). To classify the responses into different “topics”, I decided to use BERTopic, a modern topic modeling approach based on transformer embeddings. This approach also allowed me to pre-specify topics based on prior research, making it particularly useful for theory-guided research where you already have an idea of what’s in the text. READ MORE HERE.\n\n\nShiny advent calendar\nPh.D. students do not have tremendous purchasing power and my partner and I had to live in different places for a year. However, I knew that I would come back to Durham, NC (where she lived at the time) and we would spend spring and summer together. To make the distance and wait more bearable, I created a Shiny app that served as an advent calendar for us. Each day, a new suggestion for a shared “date activity” would pop up, like a boat rental or a nice restaurant. It was a fun way to combine my coding skills with a personal touch, and it made the holiday season special despite the distance. READ MORE HERE.\n\n\nTaylor Swift lyrics\nI can’t say that I have been a Taylor Swift fan since her early days. However, I will readily admit that her songwriting does a tremendous job at capturing the lived experience of a Millenial and as she aged her songs also matured (and there are all these fun Shakespeare references). I was curious whether I could find patterns in her lyrics that correspond to different “eras” of her music career. The data were readily available, neatly wrapped in an R package, so I processed the text, and went to town with various NLP techniques to see if distinct themes or styles emerged over time. READ MORE HERE.\n\n\nRAG of teaching materials\nTo help my students prepare for their final papers and making a “more targeted” GPT for them, I wanted to create a Retrieval-Augmented Generation (RAG) system that could answer their questions based on the materials I provided throughout the semester. This involved collecting lecture notes, slides, and reading materials, casting them into a machine-readable format, and then setting up a RAG pipeline that could retrieve relevant information, coupled with a local LLM to generate coherent answers. The goal was to make studying more interactive and efficient for my students. READ MORE HERE.\n\n\nTranscription tutorial\nSocial scientists use plenty of text data for their research due to the fact that its readily available and easily analyzable. However, an in my experience often overlooked source of text data are audio or video recordings. Interviews, focus groups, speeches, and even podcasts can be treasure troves of data. To help fellow researchers get started with transcribing audio data into text, I created a tutorial that walks through the process using popular tools and services. The tutorial covers everything from speaker diarization with pyannote (for instance, if you have interview or focus group recordings) to using a large model in the background (OpenAI Whisper) to ensure high accuracy in your transcriptions. Unlike tools, such as Zoom’s transcription software, my approach is fully free and the data will not be used for training the providers’ models – making it well-suited for sensitive data. READ MORE HERE.\n\n\nUsing LLMs for coding qualitative data\nQualitative data analysis often involves coding text data into different categories or themes. This can be a time-consuming and subjective process. To explore how large language models (LLMs) can assist in this task, I set up an experiment where I used an LLM to code a set of qualitative data and compared its performance to a human coder (me). The goal was to assess the accuracy, consistency, and efficiency of using LLMs for qualitative coding, and to identify potential benefits and limitations of this approach. The approach I suggest works across languages (depending on LLM choice), is fairly quick and robust. Also, it runs locally (at least on my 2022 MacBook Pro M1 Pro), thus the data will never be shared with other companies (e.g., OpenAI, Anthropic). READ MORE HERE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Website",
    "section": "",
    "text": "Hi! My name is Felix and this is my personal website. I am a final-year Ph.D. student in Sociology at the Ecole Polytechnique in Paris. I live in Boston with my partner (who is doing her Ph.D. in Sociology, too) and two gorgeous cats. I am a German citizen and have a valid work permit for the U.S.\nI spent years doing research in the field of cultural and political sociology, asking big questions about human behavior and waiting even longer for answers. This career gave me great opportunities: I lived in Germany, Sweden, France, and the U.S., I met great and interesting people, and learned a lot. However, between endless research timelines, publishing and revision cycles, and the slow churn of academic life, I also realized that I was craving something different: the chance to see my work make an impact more immediately and directly. Who cares about a ten-page journal article that gets 1,000 downloads and maybe influences a few other researchers if it doesn’t change anything in the real world?\nThe closest you can get to experiencing impact in academia is through teaching and conference visits. I have been teaching younger folks how to go after social science questions in R and Python throughout my entire academic career (starting right after graduating from my Bachelor’s program) and also attended and presented at more than 10 academic conferences. These occasions showed me what I love most about this work: solving problems and telling stories with data. There is nothing I enjoy more than the moment when a visualization clicks, when a pattern emerges from messy data, when numbers suddenly tell a story that changes how someone sees a problem. I want to do that outside the university, with data that’s as exciting and dynamic as the questions it can answer.\nThat desire for real-world impact led me to work as a consulting analyst for Statistical Horizons, where I got my first taste of what data work looks like outside academia. I crunched performance data for reports and dashboards, prepared marketing campaigns using customer data that I collected online, and created reports that made an immediate and measurable impact. It confirmed what I suspected: I want to do this full-time.\nSo I am looking for data analyst and data science roles where I can combine rigorous methodology with practical problem-solving. Where I can dive into rich datasets, uncover insights that actually shift decisions, and communicate findings in ways that resonate with and help the people who need them.\nThis portfolio is a glimpse of how I work – curious, methodical, sometimes a bit outside the box, but always driven by a desire to understand and explain. If you like what you see here, please reach out. I would love to connect and explore how I can bring this approach to your team."
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html",
    "href": "projects/boston_marathon_procrastination.html",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "",
    "text": "You know you’ve reached peak Ph.D. thesis avoidance when you decide that scraping 1000+ Strava activities from the Boston Marathon seems like a totally reasonable use of your time. But here we are.\nInstead of writing about my actual research, I was wondering what kind of gear folks wear at the Boston Marathon and whether you could see some correlations with their Relative Exertion and finishing times.\nThe Strava API didn’t prove to be very generous, so I built a little web scraper. I love web scraping, it’s a bit like solving a little sudoku, and every website holds some new challenges."
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#step-1-finding-the-segment",
    "href": "projects/boston_marathon_procrastination.html#step-1-finding-the-segment",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Step 1: Finding the Segment",
    "text": "Step 1: Finding the Segment\nThe first half of the Boston Marathon course exists as Segment 12666537 on Strava. I did not go for a longer segment, since some people might choose to end their activity early or so. I will, of course, also miss folks who don’t start their watch right at the start or whose watches have GPS glitches. However, for 2025, around 6,000 segment times were recorded on this approximately 21km segment.\nSeeing full Strava segment lists requires you to be a summit user, so I had to log into my account. Also, it’s a dynamic website. So, I ended up using selenium for data acquisition.\n\n\nShow the code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\nimport random\nimport time\nimport re\n\n# opens a Firefox window\ndriver = webdriver.Firefox()\n\n# Navigate to the website\ndriver.get(\"https://www.strava.com/segments/12666537?filter=overall\") \n## go through log in procedure manually in the browser and select segment efforts for 2025"
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#step-2-collecting-activity-urls",
    "href": "projects/boston_marathon_procrastination.html#step-2-collecting-activity-urls",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Step 2: Collecting Activity URLs",
    "text": "Step 2: Collecting Activity URLs\nFirst, I needed to gather all the activity URLs. The segment leaderboard is paginated, so I wrote a loop to click through pages and collect links. I added random delays because I’m not a monster and the website had some latency (the latter could have also been achieved – and made more robust – by asking selenium to wait until the website was done building).\n\n\nShow the code\nactivity_urls = []\n\ndef get_links():\n    specific_links = driver.find_elements(By.CSS_SELECTOR, \".track-click:nth-child(3) a\")\n    specific_urls = [link.get_attribute(\"href\") for link in specific_links]\n    return specific_urls\n\ni = 0\nwhile TRUE:\n    activity_urls.append(get_links())\n    time.sleep(random.uniform(5, 10))\n    next = driver.find_element(By.CSS_SELECTOR, \".next_page a\")\n    next.click()\n\ntemp = pd.DataFrame({'url': activity_url_list})\ntemp.to_csv(\"files/boston_m_urls_2025.csv\")"
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#step-3-scraping-individual-activities",
    "href": "projects/boston_marathon_procrastination.html#step-3-scraping-individual-activities",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Step 3: Scraping Individual Activities",
    "text": "Step 3: Scraping Individual Activities\nThis is where things got interesting (read: tedious). For each activity URL, I needed to:\n\nNavigate to the activity\nClick on the “Overview” tab\nExtract stats (distance, pace, time)\nGrab device and shoe information\nNot get blocked by Strava (hence the generous random delays)\n\nMy first attempt had some retry logic for when elements didn’t load properly (the site proved to be fairly sketchy and unstable):\n\n\nShow the code\nfor url in current_urls:\n    time.sleep(random.uniform(20, 60))\n    driver.get(url)\n    time.sleep(random.uniform(2, 9))\n    overview = driver.find_element(By.LINK_TEXT, \"Overview\")\n    max_attempts = 4\n    attempts = 0\n    stats = ['']\n    gear = ['']\n    \n    while attempts &lt; max_attempts and (len(stats[0]) &lt; 1 or len(gear[0]) &lt; 1):\n        overview.click()\n        time.sleep(random.uniform(2, 10))\n        stats = driver.find_elements(By.CSS_SELECTOR, \".inline-stats\")\n        stats = [element.text for element in stats]\n        gear = driver.find_elements(By.CSS_SELECTOR, \".device-section\")\n        gear = [element.text for element in gear]\n        attempts += 1\n    date = driver.find_elements(By.CSS_SELECTOR, \"time\")\n    date = [element.text for element in date]\n    temp = pd.DataFrame({\n    'date': date[0],\n    'run_data': stats,\n    'gear': gear\n    })\n    result = pd.concat([result, temp], ignore_index=True)\n\n\nEventually, I refined my approach to directly construct the overview URL using a RegEx which stabilized things. I hit a couple of runtime errors, so in the end I stopped my collection after getting the ~1000 runners who had finished the first half Strava segment fastest:\n\n\nShow the code\nfor url in current_urls:\n    time.sleep(random.uniform(3, 10))\n    driver.get(url)\n    time.sleep(random.uniform(2, 9))\n    current_url = driver.current_url\n    new_url = re.sub(r'segments.*', 'overview', current_url)\n    if new_url == current_url:\n        new_url = re.sub(r'#.*', '/overview', current_url)\n    driver.get(new_url)\n    time.sleep(random.uniform(2, 5))\n    stats = driver.find_elements(By.CSS_SELECTOR, \".inline-stats\")\n    stats = [element.text for element in stats]\n    gear = driver.find_elements(By.CSS_SELECTOR, \".device-section\")\n    gear = [element.text for element in gear]\n    date = driver.find_elements(By.CSS_SELECTOR, \"time\")\n    date = [element.text for element in date]\n    temp = pd.DataFrame({\n    'date': date[0],\n    'run_data': stats,\n    'gear': gear,\n    'url' : new_url\n    })\n    result = pd.concat([result, temp], ignore_index=True)\n\nresult.to_csv(\"files/strava_results.csv\")\n\n\nAfter several hours of watching Firefox windows open and close (JK, I read a book, the computer was fine on its own), I had my data."
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#step-4-the-fun-part-data-cleaning-in-r",
    "href": "projects/boston_marathon_procrastination.html#step-4-the-fun-part-data-cleaning-in-r",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Step 4: The Fun Part – Data Cleaning in R",
    "text": "Step 4: The Fun Part – Data Cleaning in R\nWith raw data in hand, I switched to R for cleaning and analysis. This involved a lot of string manipulation to extract meaningful information from the messy scraped text. The data looked like this:\n\n\nShow the code\nneeds(tidyverse, kableExtra, here)\n\nread_csv(here(\"files\", \"strava_results.csv\")) |&gt;\n  tail(10) |&gt;\n  mutate(across(where(is.character), \\(x) str_replace_all(x, \"\\n\", \"\\\\\\\\n\"))) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n…1\ndate\nrun_data\ngear\nurl\n\n\n\n\n1331\nMonday, April 21, 2025\n42.46 km:51:54Time:03 /km\nGarmin Forerunner 965: —\nhttps://www.strava.com/activities/14244528537/overview\n\n\n1332\nMonday, April 21, 2025\n42.41 km:48:34Time:58 /kmEffort\nCOROS PACE 3: Nike Vaporfly 3 (61.1 km)\nhttps://www.strava.com/activities/14244476616/overview\n\n\n1333\nMonday, April 21, 2025\n42.57 km:55:20Time:07 /kmRelative Effort\nGarmin Forerunner 245 Music: —\nhttps://www.strava.com/activities/14245003934/overview\n\n\n1334\nMonday, April 21, 2025\n42.53 km:53:28Time:05 /kmRelative Effort\nGarmin fēnix 6X: Saucony Endorphin Pro 4 (139.3 km)\nhttps://www.strava.com/activities/14244563533/overview\n\n\n1335\nMonday, April 21, 2025\n42.55 km:50:00Time:00 /km\nGarmin Venu 2: Nike Alphafly 3 (134.2 km)\nhttps://www.strava.com/activities/14245148177/overview\n\n\n1336\nMonday, April 21, 2025\n42.52 km:53:52Time:05 /km\nGarmin Forerunner 245 Music: —\nhttps://www.strava.com/activities/14247112004/overview\n\n\n1337\nMonday, April 21, 2025\n42.53 km:55:15Time:07 /km\nGarmin Forerunner 265: Nike 2 Vaporfly 3 (233.9 km)\nhttps://www.strava.com/activities/14244454090/overview\n\n\n1338\nMonday, April 21, 2025\n42.71 km:58:25Time:11 /km\nGarmin Forerunner 245 Music: —\nhttps://www.strava.com/activities/14244568999/overview\n\n\n1339\nMonday, April 21, 2025\n42.45 km:55:07Time:07 /kmRelative Effort\nGarmin Forerunner 165 Music: PUMA Deviate Nitro Elite 3 (370.4 km)\nhttps://www.strava.com/activities/14244721687/overview\n\n\n1340\nMonday, April 21, 2025\n42.56 km:54:44Time:06 /kmRelative Effort\nGarmin Forerunner 945 LTE: Nike Vaporfly 4 (72.5 km)\nhttps://www.strava.com/activities/14244776218/overview\n\n\n\n\n\n\n\nShow the code\nneeds(hms)\n\nboston_data &lt;- read_csv(here(\"files\", \"strava_results.csv\")) |&gt; \n  select(-1) |&gt; \n  distinct(date, run_data, gear, .keep_all = TRUE) |&gt; \n  rowid_to_column(\"rank\") |&gt; \n  separate(\n    run_data, \n    into = c(\"distance\", \n             \"scrap_1\", \n             \"elapsed_time\", \n             \"scrap_2\", \n             \"pace\", \n             \"scrap_3\", \n             \"relative_effort\", \n             \"scrap_4\"), \n    sep = \"\\\\n\"\n  ) |&gt; \n  separate(gear, into = c(\"device\", \"shoes\"), sep = \"\\\\n\") |&gt; \n  mutate(\n    pace = pace |&gt; str_extract(\"[0-9]:[0-9]{2}\") |&gt; str_c(\"00:\", x = _) |&gt; parse_hms(),\n    distance = distance |&gt; str_remove(\" km$\") |&gt; parse_double(),\n    relative_effort = parse_double(relative_effort),\n    elapsed_time = parse_hms(elapsed_time),\n    date = str_remove(date, \"^[A-Za-z]*, \") |&gt; parse_date(format = \"%B %d, %Y\")\n  ) |&gt; \n  select(rank:distance, pace, elapsed_time, relative_effort, device, shoes) |&gt; \n  filter(date == ymd(\"2025-04-21\") & distance &gt; 42)\n\n\nThis looks a bit better already:\n\n\nShow the code\nboston_data |&gt; head(5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrank\ndate\ndistance\npace\nelapsed_time\nrelative_effort\ndevice\nshoes\n\n\n\n\n1\n2025-04-21\n42.35\n00:03:00\n02:07:03\n225\nCOROS PACE Pro\nShoes: —\n\n\n2\n2025-04-21\n42.32\n00:03:00\n02:07:08\n365\nCOROS PACE Pro\nShoes: ASICS Metaspeed Sky 4 (1,433.0 km)\n\n\n3\n2025-04-21\n42.42\n00:03:00\n02:07:21\nNA\nCOROS APEX 2 Pro\nShoes: —\n\n\n4\n2025-04-21\n42.48\n00:03:04\n02:10:20\n283\nGarmin Forerunner 255S Music\nShoes: —\n\n\n5\n2025-04-21\n42.32\n00:03:01\n02:08:00\n498\nCOROS PACE 3\nShoes: —"
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#cleaning-watch-data",
    "href": "projects/boston_marathon_procrastination.html#cleaning-watch-data",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Cleaning Watch Data",
    "text": "Cleaning Watch Data\nFirst, I wanted to know what kinds of watches people used. This data is fairly clean since they come straight from the API and, as of lately, Strava by default shows the manufacturer and model. There are of course many varieties of watches, thus I aimed to reduce them to manufacturer and model category (e.g., “Garmin Forerunner” instead of “Garmin Forerunner 265 Music”).\n\n\nShow the code\nwatches &lt;- boston_data |&gt; \n  select(rank, device) |&gt; \n  filter(!str_detect(device, \"Shoe\")) |&gt; \n  mutate(\n    watch_brand = device |&gt; str_to_lower() |&gt; str_extract(\"^[a-z]*\"),\n    watch_model = device |&gt; str_to_lower() |&gt; str_remove(\"^[a-z]*\") |&gt; str_squish()\n  ) |&gt; \n  group_by(watch_brand) |&gt; \n  filter(n() &gt; 5) |&gt; \n  mutate(\n    watch_model_cat = case_when(\n      watch_brand == \"garmin\" ~ str_extract(watch_model, \"^[\\\\w]*\"),\n      watch_brand == \"coros\" ~ str_extract(watch_model, \"^[a-z]*\"),\n      watch_brand == \"apple\" & str_detect(watch_model, \"ultra\") ~ \"watch ultra\",\n      watch_brand == \"apple\" & str_detect(watch_model, \"se\\\\b\") ~ \"watch se\",\n      watch_brand == \"apple\" & str_detect(watch_model, \"watch\") ~ \"watch\",\n      watch_brand == \"suunto\" ~ str_extract(watch_model, \"^[1-9a-z]*\"),\n      watch_brand == \"polar\" ~ str_extract(watch_model, \"^[1-9a-z]*\"),\n      TRUE ~ NA_character_\n    )\n  )\n\n\nThe end result looked like this:\n\n\nShow the code\nwatches |&gt; head(5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\nrank\ndevice\nwatch_brand\nwatch_model\nwatch_model_cat\n\n\n\n\n1\nCOROS PACE Pro\ncoros\npace pro\npace\n\n\n2\nCOROS PACE Pro\ncoros\npace pro\npace\n\n\n3\nCOROS APEX 2 Pro\ncoros\napex 2 pro\napex\n\n\n4\nGarmin Forerunner 255S Music\ngarmin\nforerunner 255s music\nforerunner\n\n\n5\nCOROS PACE 3\ncoros\npace 3\npace"
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#cleaning-shoes",
    "href": "projects/boston_marathon_procrastination.html#cleaning-shoes",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Cleaning Shoes",
    "text": "Cleaning Shoes\nAnd of course, the shoes – because runners care about shoes almost as much as their splits. Here, the data are manually entered by each user and, thus, messy. However, since you are forced to choose the brand from a drop down menu, at least this part is clean.\n\n\nShow the code\nshoes &lt;- boston_data |&gt; \n  select(rank, shoes) |&gt; \n  mutate(\n    shoe_km = shoes |&gt; str_extract(\"\\\\([0-9,\\\\.]* km\\\\)\") |&gt; str_remove_all(\"[(),km]\") |&gt; parse_double(),\n    shoe_brand = shoes |&gt; \n      str_remove(r\"(Shoes: )\") |&gt; \n      str_remove(\" \\\\([0-9].*$\") |&gt; \n      str_to_lower() |&gt; \n      str_replace_all(\"new balance\", \"nb\") |&gt; \n      str_extract(\"^[a-z]*\"),\n    shoe_model = shoes |&gt; \n      str_remove(r\"(Shoes: )\") |&gt; \n      str_remove(\" \\\\([0-9].*$\") |&gt; \n      str_to_lower() |&gt; \n      str_replace_all(\"new balance\", \"nb\") |&gt; \n      str_remove(\"^[a-z]*\") |&gt; \n      str_squish()\n  ) |&gt; \n  filter(!str_length(shoe_brand) == 0) |&gt; \n  filter(shoe_km &lt; 1000) |&gt; \n  group_by(shoe_brand) |&gt; \n  filter(n() &gt; 5) |&gt; \n  ungroup()\n\n\n\n\nShow the code\nshoes |&gt; head(5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\nrank\nshoes\nshoe_km\nshoe_brand\nshoe_model\n\n\n\n\n7\nShoes: HOKA Rocket x^2 (148.8 km)\n148.8\nhoka\nrocket x^2\n\n\n9\nShoes: Brooks HPE 5 (coral) (227.3 km)\n227.3\nbrooks\nhpe 5 (coral)\n\n\n16\nShoes: ASICS Metaspeed Paris EDGE - zelene (253.1 km)\n253.1\nasics\nmetaspeed paris edge - zelene\n\n\n17\nShoes: Nike VaporFly 3 (98.4 km)\n98.4\nnike\nvaporfly 3\n\n\n19\nShoes: Adidas Adizero Adios Pro Evo 1 (42.5 km)\n42.5\nadidas\nadizero adios pro evo 1"
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#descriptives",
    "href": "projects/boston_marathon_procrastination.html#descriptives",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Descriptives",
    "text": "Descriptives\nNow I can finally get to some descriptives.\n\nSample\n\n\nShow the code\ncleaned_data &lt;- boston_data |&gt; \n  left_join(watches) |&gt; \n  left_join(shoes)\n\ncleaned_data |&gt; \n  ggplot() +\n  geom_boxplot(aes(x = elapsed_time)) +\n  theme_minimal() +\n  labs(x = \"Elapsed Time\", y = \"\")\n\n\n\n\n\n\n\n\n\nThe folks I scraped where quite fast, median elapsed time was around 2:45 hours. However, there are four outliers that started very fast but paid for it eventually. Also, the Boston Marathon qualifying times lie at around 3:30h (depending on age and gender), so we are dealing with a very fit sub-sample here.\nMost runners who started strong also finished strong. The following graphs plots their rank in the segment (i.e., their position in the field at the halfway point) and the final rank in the elapsed time (I did not scrape the exact segment time). However, some of them dearly paid for it.\n\n\nShow the code\ncleaned_data |&gt; \n  rename(rank_first_half = rank) |&gt; \n  arrange(elapsed_time) |&gt; \n  rowid_to_column(\"rank_final\") |&gt; \n  mutate(neg_split = if_else(rank_first_half &gt; rank_final, TRUE, FALSE)) |&gt; \n  ggplot() +\n  xlim(c(0, 1250)) +\n  ylim(c(0, 1250)) +\n  geom_point(aes(rank_first_half, rank_final, color = neg_split)) +\n  labs(x = \"Rank: Half-point\", y = \"Rank: Finish\", color = \"Improved Rank\")\n\n\n\n\n\n\n\n\n\nWe see that some folks really came apart badly (the red dots that are low on the x axis but high on the y axis). On the other hand, people who made up ground did only modestly do so – this makes sense, it’s easier to stand still and get passed by 100s of runners than to pass 100s of runners.\nHowever, let’s focus on their gear now.\n\n\nPopularity\nFirst of all, which shoes are the most popular in my sample (note: you can hover over the bars to get the precise number)? Strava also gave me the mileage of the shoe. IIRC, running shoes should typically be replaced every 400-800km. Hence, I remove every shoe that’s allegedly been used for more than 1,000 kilometers, presuming that the user did not properly enter the correct shoe they ran in.\n\n\nShow the code\nneeds(plotly)\n\nshoe_cat &lt;- cleaned_data |&gt; \n  add_count(shoe_brand, name = \"n\") |&gt; \n  drop_na(shoe_brand) |&gt; \n  ggplot(aes(shoe_brand, fill = shoe_brand, text = str_glue(\"{shoe_brand}, N={n}\"))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\n    RColorBrewer::brewer.pal(9, \"Set1\")\n  )) +\n  theme_minimal() +\n  labs(x = \"Shoe Brand\", y = \"N\", caption = \"Brands with n&lt;=5 were excluded.\") +\n  theme(legend.position = \"none\")\n\n\nshoe_cat |&gt; ggplotly(tooltip = \"text\")\n\n\n\n\n\n\nNike clearly dominates here.\nCan we see trends with regard to the particular models? The shoe data is messy, so my regexes don’t cut it. However, I can employ a local LLM to clean it up. The prompt I use is:\n\nYou are a running shoe expert trying to categorize running shoes. INSTRUCTIONS: extract the running shoe model from the running shoe entry. Try to keep it broad, but with enough detail. Examples: - ‘ASICS Metaspeed Sky 4’ should become ‘Metaspeed’ - ‘Adidas Adizero Adios Pro Evo 1’ should become ‘Adios Pro’ - ‘Nike Vaporfly Next% 3 - 2’ should become ‘Vaporfly’\n\nThis gives me an idea of the models but not necessarily the generation, specific colorway, etc. The LLM does an okay job at cleaning, but I still need to clean some things up with a set of regexes. A larger lookup table with different models would probably be a more efficient way of cleaning up these data.\n\n\nShow the code\nneeds(ollamar, ellmer)\nollamar::pull(\"qwen2.5:7b\")\nshoe_model &lt;- type_object(\n  model = type_string(\"extracted model\")\n)\n\nref_prompt_structured &lt;- \"\nYou are a running shoe expert trying to categorize running shoes. \n\nINSTRUCTIONS: extract the running shoe model from the running shoe entry. \n  Try to keep it broad, but with enough detail.\n\n  Examples: \n    - 'ASICS Metaspeed Sky 4' should become 'Metaspeed'\n    - 'Adidas Adizero Adios Pro Evo 1' should become 'Adios Pro'\n    - 'Nike Vaporfly Next% 3 - 2' should become 'Vaporfly'\n\"\n\nshoe_classifier &lt;- chat_ollama(\n  model = \"qwen2.5:7b\",\n  system_prompt = ref_prompt_structured,\n  params = params(\n    temperature = 0.2, # low for consistency\n    seed = 42 # Reproducible results\n  )\n)\n\nmodel_classification &lt;- boston_data |&gt; \n  select(rank, shoes) |&gt; \n  mutate(\n    shoe_km = shoes |&gt; str_extract(\"\\\\([0-9,\\\\.]* km\\\\)\") |&gt; str_remove_all(\"[(),km]\") |&gt; parse_double(),\n    shoe_brand = shoes |&gt; \n      str_remove(r\"(Shoes: )\") |&gt; \n      str_remove(\" \\\\([0-9].*$\") |&gt; \n      str_to_lower() |&gt; \n      str_replace_all(\"new balance\", \"nb\") |&gt; \n      str_extract(\"^[a-z]*\"),\n    shoe_model = shoes |&gt; \n      str_remove(r\"(Shoes: )\") |&gt; \n      str_remove(\" \\\\([0-9].*$\") |&gt; \n      str_to_lower() |&gt; \n      str_replace_all(\"new balance\", \"nb\") |&gt; \n      str_remove(\"^[a-z]*\") |&gt; \n      str_squish()\n  ) |&gt; \n  filter(!str_length(shoe_brand) == 0) |&gt; \n  filter(shoe_km &lt; 1000) |&gt; \n  dplyr::pull(shoe_model) |&gt; \n  enframe(name = NULL, value = \"x\") |&gt; \n  rowid_to_column(\"id\") |&gt; \n  pmap(\n    \\(x, id) {\n      if ((id - 1) %% 20 == 0) {\n        classifier &lt;&lt;- chat_ollama(\n          model = \"qwen2.5:7b\",\n          system_prompt = ref_prompt_structured,\n          params = params(\n            temperature = 0.2,\n            seed = 42\n          )\n        )\n      }\n      classifier$chat_structured(x, type = shoe_model)\n    },\n    .progress = TRUE\n  )\n\nmodels &lt;- model_classification |&gt; \n  bind_rows() |&gt; \n  mutate(model = str_to_lower(model) |&gt; \n    str_remove_all(\"[0-9]|next%|adizero|zoomx|air zoom\") |&gt; \n    str_squish() |&gt; \n    str_replace_all(\n      c(\"alpha fly\" = \"alphafly\",\n        \"fast.?r.*\" = \"fast-r\",\n        \"alphafly.*\" = \"alphafly\",\n        \"a fly\" = \"alphafly\",\n        \"^alpha$\" = \"alphafly\",\n        \"meta.?speed.*\" = \"metaspeed\",\n        \"^af.*$\" = \"alphafly\",\n        \"alphaphly\" = \"alphafly\",\n        \"cloud.?boom .*\" = \"cloudboom\",\n        \"^pro$\" = \"adios pro\",\n        \"adios pro evo\" = \"adios pro\",\n        \"vaporfly.*\" = \"vaporfly\",\n        \"hype elite\" = \"hyperion elite\",\n        \"cielo x.*\" = \"cielo\",\n        \"rocket x.*\" = \"rocket\",\n        \"sc\" = \"supercomp\",\n        \"fuelcell supercomp\" = \"supercomp\",\n        \"feulcell supercomp\" = \"supercomp\",\n        \"fuelcell rc\" = \"supercomp\",\n        \"nike \" = \"\",\n        \"vapor$\" = \"vaporfly\",\n        \"vf.*\" = \"vaporfly\",\n        \"deviate.*\" = \"deviate\",\n        \"endorphin.*\" = \"endorphin\",\n        \"^alpha s$\" = \"alphafly\",\n        \" v$\" = \"\")\n    ))\n\nboston_data |&gt; \n  select(rank, shoes) |&gt; \n  mutate(\n    shoe_km = shoes |&gt; str_extract(\"\\\\([0-9,\\\\.]* km\\\\)\") |&gt; str_remove_all(\"[(),km]\") |&gt; parse_double(),\n    shoe_brand = shoes |&gt; \n      str_remove(r\"(Shoes: )\") |&gt; \n      str_remove(\" \\\\([0-9].*$\") |&gt; \n      str_to_lower() |&gt; \n      str_replace_all(\"new balance\", \"nb\") |&gt; \n      str_extract(\"^[a-z]*\"),\n    shoe_model = shoes |&gt; \n      str_remove(r\"(Shoes: )\") |&gt; \n      str_remove(\" \\\\([0-9].*$\") |&gt; \n      str_to_lower() |&gt; \n      str_replace_all(\"new balance\", \"nb\") |&gt; \n      str_remove(\"^[a-z]*\") |&gt; \n      str_squish()\n  ) |&gt; \n  filter(!str_length(shoe_brand) == 0) |&gt; \n  filter(shoe_km &lt; 1000) |&gt; \n  bind_cols(models)\n\n\n\n\nShow the code\nclean_data_w_models &lt;- cleaned_data |&gt; \n  select(rank:relative_effort, watch_brand, watch_model_cat, shoe_brand) |&gt; \n  left_join(\n    read_csv(here(\"files\", \"boston_shoe_models.csv\")) |&gt; \n              select(rank, shoe_model = model)\n  )\n\n\nWhich shoe model is the most popular among the brands? I specify all models that were identified less than 5 times total as “other”.\n\n\nShow the code\nshoe_model_plot &lt;- clean_data_w_models |&gt; \n  count(shoe_brand, shoe_model) |&gt; \n  mutate(shoe_model = case_when(\n    n &gt; 4 ~ shoe_model,\n    TRUE ~ \"other\"\n  )) |&gt; \n    drop_na(shoe_brand, shoe_model) |&gt; \n    group_by(shoe_brand, shoe_model) |&gt; \n    summarize(n = sum(n)) |&gt; \n    ggplot(aes(x = shoe_brand, y = n, fill = shoe_model, text = str_glue(\"{shoe_model}, N={n}\"))) +\n    geom_col() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\nshoe_model_plot |&gt; ggplotly(tooltip = \"text\")\n\n\n\n\n\n\nWe can also summarize this plot into two tables with percentages.\n\n\nShow the code\ntotal_n &lt;- nrow(clean_data_w_models |&gt; drop_na(shoe_brand))\nbrand_share &lt;- clean_data_w_models |&gt; \n  count(shoe_brand, shoe_model) |&gt; \n  mutate(shoe_model = case_when(\n    n &gt; 4 ~ shoe_model,\n    TRUE ~ \"other\"\n  )) |&gt; \n    drop_na(shoe_brand, shoe_model) |&gt; \n    group_by(shoe_brand, shoe_model) |&gt; \n    summarize(n = sum(n)) |&gt; \n  group_by(shoe_brand) |&gt; \n  summarize(`Share (%)` = 100*(sum(n)/total_n) |&gt; round(3)) |&gt; \n  rename(Brand = shoe_brand)\n\nmost_pop_models &lt;- clean_data_w_models |&gt; \n  drop_na(shoe_brand) |&gt; \n  mutate(shoe = str_c(shoe_brand, \" \", shoe_model)) |&gt; \n  count(shoe) |&gt; \n  group_by(shoe) |&gt; \n  summarize(`Share (%)` = 100*(sum(n)/total_n) |&gt; round(3)) |&gt; \n  slice_max(`Share (%)`, n = 10) |&gt; \n  rename(Shoe = shoe)\n\n\n\n\nShow the code\ntotal_n |&gt; kable()\n\n\n\n\n\nx\n\n\n\n\n564\n\n\n\n\n\n\n\nShow the code\nmost_pop_models |&gt; kable()\n\n\n\n\n\nShoe\nShare (%)\n\n\n\n\nnike alphafly\n30.7\n\n\nnike vaporfly\n16.0\n\n\nadidas adios pro\n11.9\n\n\nasics metaspeed\n10.1\n\n\nsaucony endorphin\n9.4\n\n\npuma fast-r\n2.3\n\n\nnb fuelcell supercomp elite\n2.1\n\n\non cloudboom\n2.1\n\n\npuma deviate\n2.0\n\n\nhoka rocket\n1.6\n\n\n\n\n\nAnd how about the watches? Here, you can look at the different models by hovering over the subsections of the bars.\n\n\nShow the code\nwatch_model_cat &lt;- cleaned_data |&gt; \n  add_count(watch_brand, watch_model_cat, name = \"n\") |&gt; \n  drop_na(watch_brand) |&gt; \n  ggplot(aes(watch_brand, fill = watch_model_cat, text = str_glue(\"{watch_model_cat}, N={n}\"))) +\n  geom_bar() +\n  #scale_y_log10(labels = scales::label_number()) +\n  scale_fill_manual(values = c(\n    RColorBrewer::brewer.pal(9, \"Set1\"),\n    RColorBrewer::brewer.pal(8, \"Dark2\"),\n    RColorBrewer::brewer.pal(8, \"Set2\")\n  )) +\n  theme_minimal() +\n  labs(x = \"Watch Brand\", y = \"N\", caption = \"Brands with n&lt;=5 were excluded.\") +\n  theme(legend.position = \"none\")\n\n\nwatch_model_cat |&gt; ggplotly(tooltip = \"text\")\n\n\n\n\n\n\nGarmin clearly dominates here, with Coros being in second place. Interestingly, there’s no Wahoo at all, despite sponsoring some runners.\n\n\nElapsed time\nDo we see some trends with regard to overall elapsed time?\n\n\nShow the code\ncleaned_data |&gt; \n  drop_na(shoe_brand) |&gt; \n  ggplot() +\n  geom_boxplot(aes(shoe_brand, elapsed_time, fill = shoe_brand)) +\n  scale_fill_manual(values = c(\n    RColorBrewer::brewer.pal(9, \"Set1\")\n  )) +\n  theme_minimal() +\n  labs(x = \"Shoe Brand\", y = \"Elapsed Time\")  +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nNo clear trends here either.\nMaybe faster runners wear different watches, e.g., due to sponsorship agreements?\n\n\nShow the code\ncleaned_data |&gt; \n  drop_na(watch_brand) |&gt; \n  ggplot() +\n  geom_boxplot(aes(watch_brand, elapsed_time, fill = watch_brand)) +\n  scale_fill_manual(values = c(\n    RColorBrewer::brewer.pal(9, \"Set1\")\n  )) +\n  theme_minimal() +\n  labs(x = \"Watch Brand\", y = \"Elapsed Time\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHere, it is worth noting that some brands were hardly present in the sample (i.e., Polar, Suunto). However, it appears as though faster runners tend to have dedicated running watches (i.e., not an Apple Watch).\nLet’s delve a bit deeper into the Garmin Forerunner universe. Upon first glance when I checked the data, it seemed as though faster runners tended to have worse watches. Is this true?\nLet’s look at the shares first:\n\n\nShow the code\ncleaned_data |&gt; \n  filter(watch_model_cat == \"forerunner\") |&gt; \n  mutate(forerunner = str_extract(watch_model, \"[0-9]{2,3}\") |&gt; as.integer()) |&gt; \n  count(forerunner) |&gt; \n  slice_max(n, n = 10) |&gt; \n  kable()\n\n\n\n\n\nforerunner\nn\n\n\n\n\n245\n126\n\n\n965\n101\n\n\n255\n97\n\n\n955\n89\n\n\n265\n62\n\n\n945\n52\n\n\n55\n37\n\n\n935\n29\n\n\n165\n24\n\n\n235\n21\n\n\n\n\n\nThe 2XX models are definitely the most popular, followed by the flagship 9XX series. It’s worth noting that the 245 was released in 2019, whereas the 965 was released in 2023 (and is still the latest Forerunner model). Maybe some fast runners have just been running for a long time and not changed their watch?\nNow we can plot the times. To simplify the plot, I just use the first number of the series (starting with 0 for the most affordable series):\n\n\nShow the code\ncleaned_data |&gt; \n  filter(watch_model_cat == \"forerunner\") |&gt; \n  mutate(forerunner = str_extract(watch_model, \"[0-9]{2,3}\"),\n         series = case_when(\n          str_length(forerunner) == 2 ~ str_c(0, forerunner),\n          TRUE ~ forerunner\n         ) |&gt; str_replace(\"[0-9]{2}$\", \"xx\")\n         ) |&gt;\n  ggplot() +\n  geom_boxplot(aes(x = series, y = elapsed_time, fill = series)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIt seems as though the really fast runners tend to wear 2XX and 9XX. However, people who wear the most affordable watches have the fastest median times in the graph. But these are of course mere tendencies.\n\n\nRelative Effort\nFinally, let’s look at Relative Effort (RE). Generally speaking, it is a score that depends on the time you spent in particular heart rate zones. Higher scores imply that the runner spent more time in higher heart rate zones. However, according to Strava, the Perceived Exertion (which a runner can enter manually) also affects RE, making it a quite unreliable measure.\nPerhaps, some shoes are bouncier and thus lead to less cardiovascular work, resulting in a lower relative effort?\n\n\nShow the code\ncleaned_data |&gt; \n  drop_na(shoe_brand) |&gt; \n  ggplot() +\n  geom_boxplot(aes(shoe_brand, relative_effort, fill = shoe_brand)) +\n  scale_fill_manual(values = c(\n    RColorBrewer::brewer.pal(9, \"Set1\")\n  )) +\n  theme_minimal() +\n  labs(x = \"Shoe Brand\", y = \"Relative Effort\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nWe see that RE values are all over the map, this might be due to people setting their heart rate zones incorrectly, rating their efforts as 10/10 in terms of perceived exertion, or really just suffering like dogs. But let’s for now just assume that RE is a good measure and not biased by user input.\nAnother factor would be running watches that measure heart rate inaccurately.\n\n\nShow the code\ncleaned_data |&gt; \n  drop_na(watch_brand) |&gt; \n  ggplot() +\n  geom_boxplot(aes(watch_brand, relative_effort, fill = watch_brand)) +\n  scale_fill_manual(values = c(\n    RColorBrewer::brewer.pal(9, \"Set1\")\n  )) +\n  theme_minimal() +\n  labs(x = \"Watch Brand\", y = \"Relative Effort\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHere, no striking differences emerge (and bear in mind that we are operating on very few data points for some of the brands).\n\n\nShoe effects\nLet’s finally go after the shoe effect on relative effort – on the brand level. To this end, we can match runners that have similar times but wear different shoes. I use Nike as my reference category and match runners who wear one of the 3 most popular brands (Adidas, Asics, Saucony). Since the overwhelming majority of runners wears Garmin watches, I focus on this group to even out differences in HR sensors.\n\n\nShow the code\nneeds(MatchIt)\n\nre_shoes &lt;- cleaned_data |&gt; \n  filter(shoe_brand %in% c(\"nike\", \"asics\", \"adidas\", \"saucony\"),\n         watch_brand == \"garmin\") |&gt;\n  mutate(\n    is_nike = if_else(shoe_brand == \"nike\", 1, 0),\n    elapsed_time_num = as.numeric(elapsed_time)\n  ) |&gt; \n  rename(distance_run = distance)\n\n# Match on elapsed time and watch brand\nmatch_shoes &lt;- matchit(\n  is_nike ~ elapsed_time_num,\n  data = re_shoes,\n  method = \"nearest\",\n  distance = \"glm\",\n  ratio = 1,\n  caliper = 300 # only considering runners that are within 5 minutes=300seconds  \n)\n\n\nNow the algorithm has paired runners into groups of two people that are similar with regard to their elapsed time and pace and have the same watch. However, only one of them wore Nike shoes, the other one did not. Thus, we have an artificial treatment and control group, if one will. Let’s have a look if there are differences.\n\n\nShow the code\nmatched_pairs &lt;- match.data(match_shoes) |&gt;\n  drop_na(relative_effort) |&gt; \n  arrange(subclass, is_nike) |&gt;\n  group_by(subclass) |&gt;\n  filter(n() == 2) |&gt; \n  select(subclass, is_nike, relative_effort) |&gt;\n  pivot_wider(\n    names_from = is_nike,\n    values_from = relative_effort,\n    names_prefix = \"re_\"\n  )\nmatched_pairs |&gt;\n  mutate(difference = re_1 - re_0) |&gt;\n  ggplot(aes(x = difference)) +\n  geom_histogram(bins = 30, fill = \"#E85D04\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = mean(matched_pairs$re_1 - matched_pairs$re_0), \n             color = \"red\", linewidth = 1.5) +\n  labs(\n    title = \"Paired Differences: Nike - Other Brands\",\n    subtitle = paste0(\"Mean difference = \", round(mean(matched_pairs$re_1 - matched_pairs$re_0), 1)),\n    x = \"Difference in Relative Effort (Nike - Other)\",\n    y = \"Count\",\n    caption = \"Red line = mean difference, Dashed line = no difference\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSeems as though folks who wear Nike shoes have less RE. But is this significant? Let’s run a t-test.\n\n\nShow the code\nt_result &lt;- t.test(\n  matched_pairs$re_1,   # Nike\n  matched_pairs$re_0,   # Non-Nike\n  paired = TRUE\n)\n\nt_result\n\n\n\n    Paired t-test\n\ndata:  matched_pairs$re_1 and matched_pairs$re_0\nt = -0.30148, df = 58, p-value = 0.7641\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -59.17431  43.68279\nsample estimates:\nmean difference \n      -7.745763 \n\n\nHeck no. The p value tells me that if there truly were no difference between Nike and other brands, we would observe a difference as large as (or larger than) what we saw in about 76% of random samples."
  },
  {
    "objectID": "projects/boston_marathon_procrastination.html#reflections",
    "href": "projects/boston_marathon_procrastination.html#reflections",
    "title": "Analyzing a Sample of Strava Data on the Boston Marathon",
    "section": "Reflections",
    "text": "Reflections\nWhat can we take home from this? Garmin dominates the market, it’s not even close. The Forerunner is clearly the most successful among runners. Also, Nike dominates the shoe market. Looking at RE did not produce any insights. Perhaps with more and cleaner data (i.e., same devices, heart rate straps, validated heart rate zones, standardized user input), better comparisons could have been possible. But this would also require Strava opening up about how the algorithm exactly works."
  },
  {
    "objectID": "research_articles/mobility.html",
    "href": "research_articles/mobility.html",
    "title": "What Do People Actually Compare When Thinking About Intergenerational Mobility?",
    "section": "",
    "text": "When we study social mobility – whether people end up better or worse off than their parents – extant research tells us that folks typically focus on education, occupation, and income. These are important, but my co-authors and I kept wondering: is that what people actually think about when they compare themselves to their parents?\nTraditional mobility research imposes our categories on people. Qualitative research lets people speak for themselves but involves small samples that can’t be compared across countries. Hence, in this project, my collaborators and I asked: what if we could combine both approaches; the scale of survey research with the openness of qualitative inquiry?"
  },
  {
    "objectID": "research_articles/mobility.html#what-we-did",
    "href": "research_articles/mobility.html#what-we-did",
    "title": "What Do People Actually Compare When Thinking About Intergenerational Mobility?",
    "section": "What We Did",
    "text": "What We Did\nWe surveyed nearly 5,000 people across Germany, Sweden, and the UK with one simple open-ended question: “When you compare how well you have done in life with your parents, what is the most important thing that you compare?”\nRespondents could write as much or as little as they wanted. We got back responses in three languages (German, English, and Swedish) with a median length of 4 words and a mean of 7.3 words. Short texts. Multiple languages. Nearly 5,000 responses.\nThis presented exactly the kind of challenge that traditional topic modeling approaches struggle with. Bayesian mixed-membership models like Structural Topic Models work by treating documents as “bags of words” and identifying topics as words that frequently co-occur within documents. But when your documents are only 4-7 words long, you don’t get much co-occurrence. And when you’re working across three languages, simple word-matching approaches fail to recognize that “income,” “Einkommen,” and “inkomst” refer to the same concept.This created a methodological challenge for me – as I was in charge of the analysis: how do you systematically analyze thousands of brief, multilingual text responses?\nI opted to use BERTopic, a machine learning approach that understands semantic meaning across languages. Unlike traditional topic modeling (which counts word co-occurrence), BERTopic uses contextual embeddings to recognize that “income,” “Einkommen,” and “inkomst” refer to the same concept even when they don’t appear in the same documents.\nThe pipeline is as follows: SBERT embedded responses in 512-dimensional semantic space \\(\\rightarrow\\) UMAP reduced dimensionality \\(\\rightarrow\\) HDBSCAN identified clusters \\(\\rightarrow\\) c-TF-IDF generated interpretable topic labels.\nWe started with fully inductive exploration (79 topics), then used theory-informed seed phrases for semi-supervised classification into 12 final categories."
  },
  {
    "objectID": "research_articles/mobility.html#what-we-found",
    "href": "research_articles/mobility.html#what-we-found",
    "title": "What Do People Actually Compare When Thinking About Intergenerational Mobility?",
    "section": "What We Found",
    "text": "What We Found\nConventional measures matter, but they’re only half the story. Yes, income, education, and occupation appeared prominently. But combined, they accounted for less than half of what people mentioned.\nHousing dominates UK responses. Home ownership concerns were 3x more prevalent in the UK than Germany or Sweden – a methodological validation that our approach captures real contextual differences.\nThe categories vary systematically. Swedes emphasized education comparisons; Germans focused on freedom and lifestyle; women highlighted education and family while men emphasized income; upwardly mobile people mentioned education and opportunities while downwardly mobile people focused on housing and economic instability.\nEmerging dimensions matter. Family life, economic security, lifestyle/freedom, well-being, climate concerns, and social relationships all appeared as distinct categories. A person might have more education and income than their parents but still feel downwardly mobile due to housing insecurity or precarity—and our method captures that."
  },
  {
    "objectID": "research_articles/mobility.html#takeaways",
    "href": "research_articles/mobility.html#takeaways",
    "title": "What Do People Actually Compare When Thinking About Intergenerational Mobility?",
    "section": "Takeaways",
    "text": "Takeaways\nMy biggest takeaway isn’t a finding but a conviction about methods: we don’t have to choose between qualitative richness and quantitative rigor. Computational text analysis, done thoughtfully, bridges these traditions.\nThe iterative approach mattered tremendously in bridging this gap: inductive exploration revealed what people actually talked about, then theory-informed refinement made it analytically useful. Neither pure induction nor pure deduction would have worked as well.\nNot all technical approaches work for all data. We needed BERTopic specifically because:\n\nour responses were short (and traditional topic models need longer texts)\nthey were multilingual (requiring semantic embeddings, not word matching)\nwe wanted both exploration and validation (benefiting from the semi-supervised workflow).\n\nFor broader research applications: when you want to know what people think without constraining their responses, open-ended questions + computational text analysis + representative sampling gives you something neither purely qualitative nor purely quantitative methods can provide alone. The barrier between these traditions keeps getting lower—we should take advantage of that.\nReality is almost always more complex than our theories predict. Sometimes our measurement can reflect that complexity."
  },
  {
    "objectID": "research_articles/bundles.html",
    "href": "research_articles/bundles.html",
    "title": "What 11,000 Swedish Twitter Users Taught Me About Lifestyle Polarization",
    "section": "",
    "text": "We’re all familiar with the cultural divides in American politics: the Prius-driving liberal versus the pickup-driving conservative. But my co-authors and I wondered – is this phenomenon unique to the American two-party system, or does it show up in countries with more diverse political landscapes? To find out, we turned to Sweden, a multi-party democracy where voters choose among eight parliamentary parties across the political spectrum."
  },
  {
    "objectID": "research_articles/bundles.html#our-approach",
    "href": "research_articles/bundles.html#our-approach",
    "title": "What 11,000 Swedish Twitter Users Taught Me About Lifestyle Polarization",
    "section": "Our Approach",
    "text": "Our Approach\nI was in charge of the research design and data analysis. Thus, I started this project wondering: what data could we use to get both people’s lifestyle interests (e.g., the music they like, the blogs they enjoy reading) but also what’s their political leaning. I turned to Twitter, since users, on the one hand, are interested to follow their favorite politicians (which I can use to infer their political leaning) but also the stuff they’re interested in in their day-to-day life.\nI collected data on 11,000 politically engaged Swedish Twitter users and analyzed their following patterns using mixed membership clustering – a technique that allows people to belong to multiple communities simultaneously, capturing the messy reality of how we actually engage with content online.\nTo have my models cut through the noise, I used a clustering approach where I could pre-specify particular clusters (such as political parties). The lifestyle interest cluster was pre-specified with interests I found by applying an inductive model. This yielded clusters such as “Soccer”, “Women & LGBTQ rights”, “Entertainers”, or “Museums” which I then formalized in the constrained, pre-specified model.\nI fitted politics and lifestyle models separately. Thus, I got one distribution of “membership” of a user with a certain party (roughly, the share of politicians they follow from a given party), which we interpreted as their political leaning, and a distribution of how much interest they take in certain lifestyles.\nFinally, I could go to work and correlate these two scores in various ways and also look at which interests co-occur together using Hierarchical clustering. I further used expert-survey data to incorporate left-right position of the parties and inferred the users’ gender by looking at their profile and performing fuzzy-matching using existing dictionaries of gendered Swedish first names."
  },
  {
    "objectID": "research_articles/bundles.html#what-we-found",
    "href": "research_articles/bundles.html#what-we-found",
    "title": "What 11,000 Swedish Twitter Users Taught Me About Lifestyle Polarization",
    "section": "What We Found",
    "text": "What We Found\nPolitical affiliation predicts a lot more than just policy preferences. Swedish voters’ media consumption, cultural interests, religious views, and even their sense of humor cluster along partisan lines. Left-leaning users follow human rights organizations, environmental groups, museums, and progressive journalists. Right-leaning users follow different content entirely: finance accounts, sports, military/security topics, and conservative commentators – even when those accounts have nothing explicitly political about them. This is also hardly affected by gender.\n\n\n\nCorrelations of lifestyle interests with political position of users. The x-axis can be interpreted as left-right leaning with more negative scores indicating a stronger left-leaning.\n\n\nThe most surprising finding? Even with eight parties to choose from, Swedish Twitter users sorted themselves into just two broad camps based on their everyday interests. These camps closely mirror the left-right coalition structure in Swedish politics. The strongest patterns appeared at the political extremes, while centrist parties showed weaker cultural alignment."
  },
  {
    "objectID": "research_articles/bundles.html#why-it-matters",
    "href": "research_articles/bundles.html#why-it-matters",
    "title": "What 11,000 Swedish Twitter Users Taught Me About Lifestyle Polarization",
    "section": "Why It Matters",
    "text": "Why It Matters\nThis suggests that lifestyle polarization isn’t a quirk of the American two-party system. Even in a country with proportional representation and coalition governments, political identity has become intertwined with everyday cultural choices. We’re not just disagreeing about taxes and healthcare – we’re living in separate cultural universes.\nOur research reveals that brands operate in a politically polarized landscape whether they acknowledge it or not. Even ostensibly neutral product categories, like sports, finance, arts, entertainment, can carry implicit political associations that shape who engages with them. When brands target “environmentally conscious consumers” or “finance enthusiasts,” they’re not just targeting a hobby or interest; they’re targeting a broader cultural-political identity that includes distinct media consumption patterns, values, and social networks. The study shows this isn’t unique to America’s two-party system but happens in multi-party democracies too, meaning international brands can’t assume European or Scandinavian markets are immune to culture wars.\nFor marketers, this means traditional segmentation strategies are incomplete. Demographics and psychographics tell only part of the story. Instead, the cultural-political identity is a hidden dimension that determines whether your message resonates or repels. Influencer partnerships, product positioning, and even category choice now carry political weight. The “neutral middle” where brands can safely appeal to everyone is smaller than most marketers assume. Brands that ignore this reality risk either leaving money on the table by failing to connect with their natural audience, or accidentally alienating customers by sending mixed cultural signals. Understanding these cultural fault lines isn’t about “getting political” – it’s about understanding the actual structure of your market."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I’ve been teaching various semester-long seminars, 2-4 day workshops, and 2 hour sessions on topics related to computational methods for social scientists. Here’s a link to my current course Toolbox Computational Social Science which is geared toward graduate students in Computational Sociology at Leipzig University.\nAll of these courses were taught in English (with some German in between), using R and Python. For the semester-long courses, students had to write 10-15 page-long course papers to pass, for which I provided extensive mentorship through office hours and 1-on-1 coding sessions.\nThematically, they covered the following techniques:\nBelow, you can find a list of classes and workshops that I have taught in chronological order (latest-earliest)"
  },
  {
    "objectID": "teaching.html#past-courses",
    "href": "teaching.html#past-courses",
    "title": "Teaching",
    "section": "Past courses",
    "text": "Past courses\n\n\n\n\n\n\n\n\n\n\nPeriod\nCourse\nInstitution\nLevel\nHours\n\n\n\n\nOct 2025-Feb 2026\nToolbox CSS\nLeipzig University\nGraduate\n64h\n\n\nOct 2024-Feb 2025\nToolbox CSS\nLeipzig University\nGraduate\n64h\n\n\nApr 2024-Jul 2024\nText Mining for Social Scientists\nLeipzig University\nUndergraduate\n32h\n\n\nOct 2023-Feb 2024\nToolbox CSS\nLeipzig University\nGraduate\n32h\n\n\nJun 2023\nSummer Institute in Computational Social Science\nInstitut Polytechnique de Paris\nGraduate and postgraduate\n30h\n\n\nMay 2023\nIntroduction to Data Management and Data Visualization in R\nSciences Po Bordeaux\nGraduate and postgraduate\n10h\n\n\nApr 2023-Jul 2023\nText Mining for Social Scientists\nLeipzig University\nUndergraduate\n32h\n\n\nFeb 2023\nIntroduction to Web Scraping with R\nWorkshops for Ukraine\n-\n2h\n\n\nJun 2022\nSummer Institute in Computational Social Science\nInstitut Polytechnique de Paris\nGraduate and postgraduate\n30h\n\n\nApr 2022-Jul 2022\nText Mining for Social Scientists\nLeipzig University\nUndergraduate\n32h\n\n\nOct 2021-Feb 2022\nToolbox CSS\nLeipzig University\nUndergraduate\n32h\n\n\nAug 2021-Sep 2021\nIntroduction to Data Management and Data Visualization in R\nStockholm School of Economics\nGraduate\n14h\n\n\nOct 2020-Feb 2021\nQuantitative Methods for Social Scientists\nUniversity of Regensburg\nGraduate\n32h\n\n\nOct 2020-Feb 2021\nBig Data Analysis with R\nUniversity of Regensburg\nUndergraduate\n32h\n\n\nFeb 2020-Jul 2020\nSocial Network Analysis\nUniversity of Regensburg\nUndergraduate\n32h\n\n\nFeb 2020-Jul 2020\nBig Data Analysis with R\nUniversity of Regensburg\nUndergraduate\n32h\n\n\nFeb 2019-Jul 2019\nSocial Network Analysis\nUniversity of Regensburg\nUndergraduate\n32h"
  },
  {
    "objectID": "teaching.html#mentorship",
    "href": "teaching.html#mentorship",
    "title": "Teaching",
    "section": "Mentorship",
    "text": "Mentorship\nI also applied my knowledge when mentoring Bachelor’s and Master’s students during their final theses. These typically applied text analysis strategies to large-scale data."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Academic Research",
    "section": "",
    "text": "What I like about the social sciences is that instead of just building models to predict outcomes, we get to dig into the why behind human behavior. Given that we all witness human behavior all the time, everyone has their own little theories of why people do what they do. The challenge of social science research is to move beyond this anecdotal evidence and figure out what actually drives behavior on a larger scale. The crux is then to communicate these insights in a way that is understandable and useful to others. On this page, I share some of the academic research projects that I conducted during my Ph.D.\n\nMediated Cues. How Elite Polarization is Transported Through the Media\nwith Väinö Yrjänäinen and Måns Magnusson\nDo people become more polarized because politicians say polarizing things in the news, or do politicians say polarizing things because people are already polarized? We studied 25 years of French newspaper coverage (n=1,000,000+ articles) and public opinion surveys to find out. The answer: both happen, but it depends on the issue. For some topics, media coverage of elite politicians clearly drives public opinion. For others, politicians seem to follow where public opinion is already going. This means there’s no one-size-fits-all explanation for how political polarization spreads. READ MORE HERE.\n\n\nHow Elite Negativity Shapes Voter Affect: Evidence from the 2021 German Federal Election\nsolo-authored; currently under review at German Politics\nWhen a political party attacks its opponents on Twitter, does it make voters dislike those opponents more? Using transformer models and OLS regression, I analyzed tweets from candidates in Germany’s 2021 election alongside biweekly voter survey data to find out. The answer is yes: when Party A criticizes Party B on social media, Party A’s supporters report disliking Party B more. The effect is strongest among people who already identify strongly with their party, and it holds up regardless of whether the parties might form a coalition together or how ideologically similar they are. Also, the opposite is not true: a party don’t criticize other parties more if they know that their supporters don’t like these parties. READ MORE HERE.\n\n\nDo Liberals Drive Volvos Everywhere? Assessing Cultural Bundles in Sweden\nwith Anastasia Menshikova, Elida Izani Binti Ibrahim, and Miriam Hurtado Bodell; currently under review at Social Media+Society\nIn the U.S., we know that political views often cluster with lifestyle choices – like the stereotype that liberals drive Priuses. But does this happen in countries with more than two major parties? We analyzed what 12,000 politically active Swedish Twitter users follow using mixed membership clustering to see if political affiliation predicts their cultural tastes. It does. Swedish voters’ media consumption, cultural interests, religious views, and even sense of humor align with their partisan identity. Political polarization isn’t just about policy disagreements – it’s seeping into everyday lifestyle choices, even in a multi-party democracy like Sweden. READ MORE HERE.\n\n\nMedia Slant as Political Refraction. Measuring Political Media Slant and Polarization in the French Media Landscape\nwith Rubing Shen, Arnault Chatelain, and Etienne Ollion\nHow can we measure whether a newspaper leans left or right without relying on subjective judgments? We developed a new method that analyzes how journalists talk about political issues differently than politicians do. By looking at subtle differences in word choice and framing, we can detect political bias at a very fine level, down to individual paragraphs. We tested this on major French newspapers from 2000-2010 and found that mainstream media became increasingly polarized during this period. This gives researchers and the public a tool to track media bias objectively over time.\n\n\nPerceptions of Intergenerational Mobility in Germany, Sweden, and the UK: Insights from Machine-Learning Text Analysis\nwith Alexi Gugushvili and Patrick Präg; preprint; currently under review at European Journal of Sociology\nWhen people compare their lives to their parents’, what are they actually comparing? We used LLMs and clustering-algorithms to analyze thousands of open-ended survey responses from Germany, Sweden, and the UK. While traditional research focuses on income, education, and job status, we found that people think about much more: home ownership, family life, freedom, lifestyle choices, and opportunities. What matters also varies dramatically by country: Swedes emphasize education, Brits focus on housing, Germans talk about freedom and lifestyle. Gender matters too: women are more likely to mention education and family, while men focus on income and career. Understanding what people actually care about when they think about social mobility gives us a richer picture than traditional metrics alone. READ MORE HERE."
  },
  {
    "objectID": "research_articles/media_alignment.html",
    "href": "research_articles/media_alignment.html",
    "title": "How The Media Can Shape Ideological Polarization",
    "section": "",
    "text": "A commonly employed definition of ideological polarization is one of “political alignment” – Republicans despise immigration and love guns, Liberals are united in their views on climate change and social equality. We experience this in our everyday lives in political discussions with peers and when reading the news. In my dissertation research, I was wondering: where do these alignments in public opinion stem from? Especially in Europe, where news coverage tends to be less overtly partisan than in the U.S. Put differently, the question this project aimed to answer is: how do people learn “which views go together”?\nTo answer this question, my collaborators (two statistics scholars from Sweden) and I decided that we should take a step back and ask ourselves: where can people learn about these alignments? How can we measure and model this alignment process? And, finally, how can we determine whether this process has actually taken place?\nThe response we came up with was that people learn about this from the news and through discussions with peers. We cannot observe peer discussions, but we can try to quantify what folks pick up from the news by measuring the associations between parties/politicians and issues. Furthermore, we can use longitudinal survey data to document that views have actually aligned with political positioning on certain things.\nExtant research has highlighted two roles for the media. On the one hand, the media do tell people what politicians and parties think about certain issues. Hence, people can learn which views are acceptable given you have a certain political identity. On the other hand, the media also reflect opinion divisions in the public. Thus, the final research question we aimed to answer is: does media coverage drive political polarization, or does it merely reflect divisions that already exist?\nThe short answer after analyzing 25 years of French data is that is not really an “either/or” but actually a “both, but with some nuance.”"
  },
  {
    "objectID": "research_articles/media_alignment.html#what-we-did",
    "href": "research_articles/media_alignment.html#what-we-did",
    "title": "How The Media Can Shape Ideological Polarization",
    "section": "What We Did",
    "text": "What We Did\nWe identified and tracked ideological alignment for 18 political issues from 1980-2005 in French survey data. Furthermore, we used Probabilistic Word Embeddings to track the associations of those issues with political elites and parties in more than 1 million newspaper articles from Le Monde. Thus, we tracked both public opinion alignment and media coverage patterns.\nFinally, we employed a set of Mixed-Effects models to uncover which pathway (media \\(\\rightarrow\\) public opinion or public opinion \\(\\rightarrow\\) media) dominates and what could drive this."
  },
  {
    "objectID": "research_articles/media_alignment.html#our-findings",
    "href": "research_articles/media_alignment.html#our-findings",
    "title": "How The Media Can Shape Ideological Polarization",
    "section": "Our Findings",
    "text": "Our Findings\nThe feedback loop is real and roughly balanced. Media coverage linking issues to partisan politicians increases public polarization by about the same magnitude that existing public divisions drive media to cover issues as political conflicts. The effects are equal, and neither really dominates.\nIt’s about ambient exposure, not breaking news. Media influence works through accumulated coverage over months and years, not immediate reaction to recent headlines. Partisan associations form gradually, almost subconsciously, through sustained exposure. That experimental research showing immediate opinion shifts? It captures something real but misses how opinion structures that actually crystallize in the real world.\nThe inverted-U pattern nobody predicted. We find that people with moderate education levels (elementary through high school) show responses to media framing 3-4 times stronger than either the least or most educated. This is probably as the least educated aren’t sufficiently exposed to or processing political coverage. The highly educated have frameworks for critical evaluation. But that middle? They’re encountering partisan signals regularly while lacking sophisticated filters. This is your persuadable middle – and it’s larger than you think.\nIssue maturity is everything. Whether an issue already has strong partisan associations fundamentally determines the dynamic. I watched emerging issues with low baseline polarization show strong top-down media effects. Established partisan issues showed the reverse: media followed public opinion more than it shaped it."
  },
  {
    "objectID": "research_articles/media_alignment.html#takeaways",
    "href": "research_articles/media_alignment.html#takeaways",
    "title": "How The Media Can Shape Ideological Polarization",
    "section": "Takeaways",
    "text": "Takeaways\nMy biggest takeaway from this research isn’t a finding but rather the conviction that reality is almost always more complex than our first-order theories predict, and that our measurement should reflect that. When I started this project, I was convinced that its a one-way street: media shapes opinion. However, reading more on existing results and explicitly testing for them in the analyses revealed more nuance.\nFurthermore, existing work often assumes that education makes people more attentive to the news and, thus, predicts more political sophistication and better alignment. While the latter finding is true according to my data, the susceptibility for change is a more nuanced story.\nFinally, not all issues “behaved” the same way – an effect found in one case is not the same in another. This means that there is tremendous value in comparing a broad set of cases and coming up with a theoretical justification that actually explains how this variation comes about to make better predictions.\nFor industry applications, this means that measurement systems need to be desinged in such a way that they can detect bidirectional relationships, heterogeneous effects, non-linear patterns, and dynamics that vary by context. Yes, this is harder than running a simple regression or A/B test, but if you’re making strategic decisions about content, products, or campaigns worth millions of dollars, shouldn’t your measurement be as sophisticated as your operations?"
  },
  {
    "objectID": "research_articles/aff_pol.html",
    "href": "research_articles/aff_pol.html",
    "title": "How Elites’ Polarised Rhetoric Shapes Voter Affect",
    "section": "",
    "text": "In one of my dissertation articles I built a measurement system that tracks messaging-to-attitude relationships at scale, with temporal precision, and directional specificity. The approach combines collecting digital trace data (i.e., political candidates’ Tweets), transformer-based classification (enabling scale), high-frequency surveying (enabling temporal analysis), and directional modeling (revealing asymmetries).\nThis unlocks research questions previously too expensive or time-consuming to answer: Does competitive messaging work? How fast do crises affect reputation? When do partnership announcements reshape perceptions? Do attacks require response?\nThe infrastructure cost is modest: fine-tuning a language model requires around 10 hours of annotating data, 1 hour of writing code (spread over multiple iterations to find the optimal parameters), and minimal compute (in this case, my 2022 MacBook Pro). After that, classification is essentially free. If you already track attitudes through surveys or panels, adding the messaging side is straightforward.\nThe political science findings – that attacks work asymmetrically, effects emerge within weeks, and defensive messaging often fails – probably do not necessarily generalize to use cases outside the political realm. But the real value is having a method to test these dynamics in your specific context rather than assuming what works in politics works for brands, or vice versa."
  },
  {
    "objectID": "research_articles/aff_pol.html#the-political-case-study",
    "href": "research_articles/aff_pol.html#the-political-case-study",
    "title": "How Elites’ Polarised Rhetoric Shapes Voter Affect",
    "section": "The Political Case Study",
    "text": "The Political Case Study\nFor my dissertation, I tracked every cross-party attack during Germany’s 2021 federal election and matched it with voter attitudes measured every two weeks. I collected 22,828 tweets from 1,537 candidates and paired them with bi-weekly surveys of about 1,000 voters from July 2021 through February 2022.\nI used GottBERT – a German AI model trained on 145 billion words – to detect genuinely polarizing rhetoric that explicitly draws “us versus them” battle lines. This delivered time-stamped, directional measurements of who attacked whom, when, and how often – something no human coding team could achieve at this scale."
  },
  {
    "objectID": "research_articles/aff_pol.html#the-approach",
    "href": "research_articles/aff_pol.html#the-approach",
    "title": "How Elites’ Polarised Rhetoric Shapes Voter Affect",
    "section": "The Approach",
    "text": "The Approach\nMy methodological edge was temporal precision. I matched elite rhetoric to voter attitudes in two-week windows. If Survey Wave 1 happened July 1 and Wave 2 on July 15, I calculated the share of polarizing rhetoric from July 2-14 and correlated it with attitude changes between surveys."
  },
  {
    "objectID": "research_articles/aff_pol.html#the-findings",
    "href": "research_articles/aff_pol.html#the-findings",
    "title": "How Elites’ Polarised Rhetoric Shapes Voter Affect",
    "section": "The Findings",
    "text": "The Findings\nWhen your side attacks opponents, your audience views those opponents more negatively. Strong and significant. When Party A went after Party B, Party A’s supporters developed measurably worse attitudes toward Party B. They did not like their own party better, though.\nThe interesting part: When your side gets attacked, your audience doesn’t rally. I expected supporters to circle the wagons when their party was under fire or develop hostility toward attackers. Neither happened. Effects were tiny, insignificant, and pointed in the opposite direction.\nI believe three factors explain this: attention asymmetry (you’re more likely to see your own side’s messages), motivated reasoning (people discount attacks on their side but process their side’s attacks as valid), and noise normalization (audiences expect conflict, so attacks don’t trigger rally effects)."
  },
  {
    "objectID": "research_articles/aff_pol.html#the-good-news",
    "href": "research_articles/aff_pol.html#the-good-news",
    "title": "How Elites’ Polarised Rhetoric Shapes Voter Affect",
    "section": "The Good News",
    "text": "The Good News\nYou’re probably worried about the implications for the state of democracy. The good news is, elite signals also work the other way round. When Germany formed its “traffic light coalition,” supporters of coalition parties immediately showed increased warmth toward their new governing partners. Affective polarization decreased significantly within weeks. Elite signals about cooperation reshaped attitudes just as quickly as attacks did."
  },
  {
    "objectID": "research_articles/aff_pol.html#the-limitations",
    "href": "research_articles/aff_pol.html#the-limitations",
    "title": "How Elites’ Polarised Rhetoric Shapes Voter Affect",
    "section": "The Limitations",
    "text": "The Limitations\nI’m transparent: This isn’t definitive causal evidence. Survey respondents might not have been exposed to specific tweets I measured. Omitted variables could drive both messaging and attitudes. Social media might not perfectly represent broader communication patterns.\nMy defense: polarizing rhetoric on Twitter reflects broader patterns audiences encounter across channels. Journalists quote tweets, messaging coordinates across platforms, and digital communication signals what’s happening elsewhere. The temporal precision, directional specificity, and theoretical consistency provide strong suggestive evidence – but true causation would of course require experiments.\nThat’s my research contribution: not just documenting problems, but showing they’re measurable, rapid, and therefore potentially solvable – in politics and beyond."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This section is where theory meets practice. Each project here represents a different slice of what I bring to data work: collecting messy data from the web, building and comparing machine learning models, applying cutting-edge NLP techniques, creating interactive tools to make my partner happy, and making complex methods accessible to others.\nSome of these grew out of pure curiosity (what patterns emerge in song lyrics? Can we detect “eras”?), others from practical needs (how do I make my teaching materials easily searchable for my students?), and a few from different research challenges (what is the best classifier for a job? how good are LLMs actually? is political polarization really so hot?). Together, they show how I approach problems: I start with a question, pick the right tools and data for the job, and don’t stop until I’ve found an answer worth sharing.\nYou’ll see a mix of techniques here: classic machine learning, modern transformer models, web scraping, interactive dashboards, and RAG applications. But the thread that runs through all of them is the same: how can I turn raw data into understandable insights, into something people can actually understand and use.\n\nWhat we can learn about runners from Strava\nI scraped 1,000+ Strava activities from the 2025 Boston Marathon to analyze what gear runners wear and whether it correlates with performance. Using Selenium to navigate dynamic webpages and wrestling with messy user-generated data, I dove into the world of running watches shoes. Spoiler alert: Garmin dominates the watch market, Nike rules the shoe game among faster runners, and despite what the marketing says, there’s no statistically significant evidence that shoe brand affects how hard you work during a marathon. READ MORE"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Felix, a final-year Ph.D. student in Sociology at the Ecole Polytechnique in Paris, currently living in Boston with my partner and two cats. I’m a German citizen with a valid U.S. work permit.\nI hold a Bachelor’s of Arts degree in Political Science (University of Regensburg), a Master’s of Science from Linköping University, and will defend my Ph.D. in Sociology at the Ecole Polytechnique in Paris in early 2026. During my Ph.D., I also spent a year as a visiting researcher at Duke University.\nYou can find my CV below. If you want to get in touch, please reach out via email or LinkedIn."
  },
  {
    "objectID": "about.html#summary",
    "href": "about.html#summary",
    "title": "About Me",
    "section": "Summary",
    "text": "Summary\nComputational Social Scientist with 8+ years experience applying computational tools to understand human behavior. Strong technical foundation in Python, R, and SQL with proven experience in machine learning, NLP, and building data-driven solutions. Experienced working remotely and collaborating with cross-functional teams."
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\nLanguages: R, Python, SQL\nML & AI: transformers, scikit-learn, PyTorch, LLMs (OpenAI, local models), prompt engineering\nData Viz: ggplot2, RShiny\nTools: git, slurm\nMethods: Machine learning (supervised/unsupervised), statistical inference, NLP, web scraping"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About Me",
    "section": "Work Experience",
    "text": "Work Experience\n\nStatistical Horizons | Data Analyst Consultant\nPhiladelphia, PA (remote) · Oct 2024 - Aug 2025\nDeveloped AI-focused curriculum, yielding 5 new AI-focused courses. Created interactive RShiny dashboards for data-driven decision making to identify and cut under-performing offerings. Increased enrolment by developing targeted advertising campaigns.\n\n\nLeipzig University | Lecturer\nLeipzig, Germany (hybrid) · Oct 2021 - Mar 2026\nDeveloped and taught 10 courses on data science methods including web scraping, NLP, and machine learning. Co-supervised 10+ Bachelor’s and 3 Master’s theses.\n\n\nLinköping University | Data Analyst\nNorrköping, Sweden (hybrid) · Feb 2020 - Jun 2021\nAnalyzed large text datasets using machine learning classifiers. Web scraped 80+ million forum postings used in 3 academic publications and training of a Swedish LLM. Assisted in research articles for Nature Human Behavior and Science Advances.\n\n\nUniversity of Regensburg | Lecturer\nRegensburg, Germany (hybrid) · Dec 2018 - Jun 2021\nTaught 7 undergraduate and graduate-level courses on social network analysis, data wrangling, visualization, and statistical modeling in R."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nPh.D. in Sociology | École Polytechnique\nParis, France · Sep 2021 - Mar 2026\nSpecialized in computational methods for digital behavioral data analysis. Co-authored 5 papers, presented at 10+ international conferences, co-organized 2 Summer Institutes in Computational Social Science.\nDissertation: Seeing Political Polarization through a Fresh Set of AIs (defense: March 4, 2026)\n\n\nM.Sc. in Computational Social Science | Linköping University\nNorrköping, Sweden · Aug 2019 - Jul 2021\n\n\nB.A. in Political Science | University of Regensburg\nRegensburg, Germany · Oct 2014 - Mar 2019"
  },
  {
    "objectID": "about.html#additional-information",
    "href": "about.html#additional-information",
    "title": "About Me",
    "section": "Additional Information",
    "text": "Additional Information\nWork Authorization: Married to US citizen (no visa sponsorship required); authorized to work in EU (German citizen)\nAwards: Ph.D. scholarship from École Polytechnique, 2 travel grants from French National Scientific Center and German Academy for Sociology\nLanguages: German (native), English (C2), French (A2)\nResearch Stays: Duke University (Durham, NC) - Spring 2024 & Spring 2025"
  }
]